{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8693e5e8-34c4-4a64-a787-da3ea38ff34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use following command to install the correct version of torchtext\n",
    "#pip install -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html torch==1.8.2+cu102 torchtext==0.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4b53bc-980f-4683-9d07-b263e5c4f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f906152-55b2-4475-b9ae-f3c392734cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffeafb-8df3-48b1-9bbd-8e6cc29504cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196f6c53-f818-4680-aad1-0b8f5d713e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile builder.py\n",
    "\n",
    "\"\"\"Graph builder from pandas dataframes\"\"\"\n",
    "from collections import namedtuple\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype, is_categorical\n",
    "import dgl\n",
    "\n",
    "__all__ = ['PandasGraphBuilder']\n",
    "\n",
    "def _series_to_tensor(series):\n",
    "    if is_categorical(series):\n",
    "        return torch.LongTensor(series.cat.codes.values.astype('int64'))\n",
    "    else:       # numeric\n",
    "        return torch.FloatTensor(series.values)\n",
    "\n",
    "class PandasGraphBuilder(object):\n",
    "    \"\"\"Creates a heterogeneous graph from multiple pandas dataframes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Let's say we have the following three pandas dataframes:\n",
    "\n",
    "    User table ``users``:\n",
    "\n",
    "    ===========  ===========  =======\n",
    "    ``user_id``  ``country``  ``age``\n",
    "    ===========  ===========  =======\n",
    "    XYZZY        U.S.         25\n",
    "    FOO          China        24\n",
    "    BAR          China        23\n",
    "    ===========  ===========  =======\n",
    "\n",
    "    Game table ``games``:\n",
    "\n",
    "    ===========  =========  ==============  ==================\n",
    "    ``game_id``  ``title``  ``is_sandbox``  ``is_multiplayer``\n",
    "    ===========  =========  ==============  ==================\n",
    "    1            Minecraft  True            True\n",
    "    2            Tetris 99  False           True\n",
    "    ===========  =========  ==============  ==================\n",
    "\n",
    "    Play relationship table ``plays``:\n",
    "\n",
    "    ===========  ===========  =========\n",
    "    ``user_id``  ``game_id``  ``hours``\n",
    "    ===========  ===========  =========\n",
    "    XYZZY        1            24\n",
    "    FOO          1            20\n",
    "    FOO          2            16\n",
    "    BAR          2            28\n",
    "    ===========  ===========  =========\n",
    "\n",
    "    One could then create a bidirectional bipartite graph as follows:\n",
    "    >>> builder = PandasGraphBuilder()\n",
    "    >>> builder.add_entities(users, 'user_id', 'user')\n",
    "    >>> builder.add_entities(games, 'game_id', 'game')\n",
    "    >>> builder.add_binary_relations(plays, 'user_id', 'game_id', 'plays')\n",
    "    >>> builder.add_binary_relations(plays, 'game_id', 'user_id', 'played-by')\n",
    "    >>> g = builder.build()\n",
    "    >>> g.number_of_nodes('user')\n",
    "    3\n",
    "    >>> g.number_of_edges('plays')\n",
    "    4\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.entity_tables = {}\n",
    "        self.relation_tables = {}\n",
    "\n",
    "        self.entity_pk_to_name = {}     # mapping from primary key name to entity name\n",
    "        self.entity_pk = {}             # mapping from entity name to primary key\n",
    "        self.entity_key_map = {}        # mapping from entity names to primary key values\n",
    "        self.num_nodes_per_type = {}\n",
    "        self.edges_per_relation = {}\n",
    "        self.relation_name_to_etype = {}\n",
    "        self.relation_src_key = {}      # mapping from relation name to source key\n",
    "        self.relation_dst_key = {}      # mapping from relation name to destination key\n",
    "\n",
    "    def add_entities(self, entity_table, primary_key, name):\n",
    "        entities = entity_table[primary_key].astype('category')\n",
    "        if not (entities.value_counts() == 1).all():\n",
    "            raise ValueError('Different entity with the same primary key detected.')\n",
    "        # preserve the category order in the original entity table\n",
    "        entities = entities.cat.reorder_categories(entity_table[primary_key].values)\n",
    "\n",
    "        self.entity_pk_to_name[primary_key] = name\n",
    "        self.entity_pk[name] = primary_key\n",
    "        self.num_nodes_per_type[name] = entity_table.shape[0]\n",
    "        self.entity_key_map[name] = entities\n",
    "        self.entity_tables[name] = entity_table\n",
    "\n",
    "    def add_binary_relations(self, relation_table, source_key, destination_key, name):\n",
    "        src = relation_table[source_key].astype('category')\n",
    "        src = src.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[source_key]].cat.categories)\n",
    "        dst = relation_table[destination_key].astype('category')\n",
    "        dst = dst.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[destination_key]].cat.categories)\n",
    "        if src.isnull().any():\n",
    "            raise ValueError(\n",
    "                'Some source entities in relation %s do not exist in entity %s.' %\n",
    "                (name, source_key))\n",
    "        if dst.isnull().any():\n",
    "            raise ValueError(\n",
    "                'Some destination entities in relation %s do not exist in entity %s.' %\n",
    "                (name, destination_key))\n",
    "\n",
    "        srctype = self.entity_pk_to_name[source_key]\n",
    "        dsttype = self.entity_pk_to_name[destination_key]\n",
    "        etype = (srctype, name, dsttype)\n",
    "        self.relation_name_to_etype[name] = etype\n",
    "        self.edges_per_relation[etype] = (src.cat.codes.values.astype('int64'), dst.cat.codes.values.astype('int64'))\n",
    "        self.relation_tables[name] = relation_table\n",
    "        self.relation_src_key[name] = source_key\n",
    "        self.relation_dst_key[name] = destination_key\n",
    "\n",
    "    def build(self):\n",
    "        # Create heterograph\n",
    "        graph = dgl.heterograph(self.edges_per_relation, self.num_nodes_per_type)\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f26ec8-d873-41c7-8450-3589ea056edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_utils.py\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# This is the train-test split method most of the recommender system papers running on MovieLens\n",
    "# takes.  It essentially follows the intuition of \"training on the past and predict the future\".\n",
    "# One can also change the threshold to make validation and test set take larger proportions.\n",
    "def train_test_split_by_time(df, timestamp, user):\n",
    "    df['train_mask'] = np.ones((len(df),), dtype=np.bool)\n",
    "    df['val_mask'] = np.zeros((len(df),), dtype=np.bool)\n",
    "    df['test_mask'] = np.zeros((len(df),), dtype=np.bool)\n",
    "    df = dd.from_pandas(df, npartitions=10)\n",
    "    def train_test_split(df):\n",
    "        df = df.sort_values([timestamp])\n",
    "        if df.shape[0] > 1:\n",
    "            df.iloc[-1, -3] = False\n",
    "            df.iloc[-1, -1] = True\n",
    "        if df.shape[0] > 2:\n",
    "            df.iloc[-2, -3] = False\n",
    "            df.iloc[-2, -2] = True\n",
    "        return df\n",
    "    df = df.groupby(user, group_keys=False).apply(train_test_split).compute(scheduler='processes').sort_index()\n",
    "    print(df[df[user] == df[user].unique()[0]].sort_values(timestamp))\n",
    "    return df['train_mask'].to_numpy().nonzero()[0], \\\n",
    "           df['val_mask'].to_numpy().nonzero()[0], \\\n",
    "           df['test_mask'].to_numpy().nonzero()[0]\n",
    "\n",
    "def build_train_graph(g, train_indices, utype, itype, etype, etype_rev):\n",
    "    train_g = g.edge_subgraph(\n",
    "        {etype: train_indices, etype_rev: train_indices},\n",
    "        preserve_nodes=True)\n",
    "    # remove the induced node IDs - should be assigned by model instead\n",
    "    del train_g.nodes[utype].data[dgl.NID]\n",
    "    del train_g.nodes[itype].data[dgl.NID]\n",
    "\n",
    "    # copy features\n",
    "    for ntype in g.ntypes:\n",
    "        for col, data in g.nodes[ntype].data.items():\n",
    "            train_g.nodes[ntype].data[col] = data\n",
    "    for etype in g.etypes:\n",
    "        for col, data in g.edges[etype].data.items():\n",
    "            train_g.edges[etype].data[col] = data[train_g.edges[etype].data[dgl.EID]]\n",
    "\n",
    "    return train_g\n",
    "\n",
    "def build_val_test_matrix(g, val_indices, test_indices, utype, itype, etype):\n",
    "    n_users = g.number_of_nodes(utype)\n",
    "    n_items = g.number_of_nodes(itype)\n",
    "    val_src, val_dst = g.find_edges(val_indices, etype=etype)\n",
    "    test_src, test_dst = g.find_edges(test_indices, etype=etype)\n",
    "    val_src = val_src.numpy()\n",
    "    val_dst = val_dst.numpy()\n",
    "    test_src = test_src.numpy()\n",
    "    test_dst = test_dst.numpy()\n",
    "    val_matrix = ssp.coo_matrix((np.ones_like(val_src), (val_src, val_dst)), (n_users, n_items))\n",
    "    test_matrix = ssp.coo_matrix((np.ones_like(test_src), (test_src, test_dst)), (n_users, n_items))\n",
    "\n",
    "    return val_matrix, test_matrix\n",
    "\n",
    "def linear_normalize(values):\n",
    "    return (values - values.min(0, keepdims=True)) / \\\n",
    "        (values.max(0, keepdims=True) - values.min(0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0ad97b-2a6f-4ccc-84aa-9d78d240dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[dataframe] in /opt/conda/lib/python3.9/site-packages (2021.11.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/lib/python3.9/site-packages (from dask[dataframe]) (0.11.2)\n",
      "Requirement already satisfied: partd>=0.3.10 in /opt/conda/lib/python3.9/site-packages (from dask[dataframe]) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from dask[dataframe]) (21.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from dask[dataframe]) (2021.11.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from dask[dataframe]) (2.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from dask[dataframe]) (6.0)\n",
      "Requirement already satisfied: pandas>=1.0 in /opt/conda/lib/python3.9/site-packages (from dask[dataframe]) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.9/site-packages (from dask[dataframe]) (1.21.4)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->dask[dataframe]) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0->dask[dataframe]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0->dask[dataframe]) (2021.3)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.9/site-packages (from partd>=0.3.10->dask[dataframe]) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.0->dask[dataframe]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install dask[dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e91c298-e684-4225-9ab9-73e3ffe6820b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScript that reads from raw MovieLens-1M data and dumps into a pickle\\nfile the following:\\n* A heterogeneous graph with categorical features.\\n* A list with all the movie titles.  The movie titles correspond to\\n  the movie nodes in the heterogeneous graph.\\nThis script exemplifies how to prepare tabular data with textual\\nfeatures.  Since DGL graphs do not store variable-length features, we\\ninstead put variable-length features into a more suitable container\\n(e.g. torchtext to handle list of texts)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script that reads from raw MovieLens-1M data and dumps into a pickle\n",
    "file the following:\n",
    "* A heterogeneous graph with categorical features.\n",
    "* A list with all the movie titles.  The movie titles correspond to\n",
    "  the movie nodes in the heterogeneous graph.\n",
    "This script exemplifies how to prepare tabular data with textual\n",
    "features.  Since DGL graphs do not store variable-length features, we\n",
    "instead put variable-length features into a more suitable container\n",
    "(e.g. torchtext to handle list of texts)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ad59ae-3f93-4292-ab35-629640d3b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "import dgl\n",
    "import torch\n",
    "#import torchtext\n",
    "from builder import PandasGraphBuilder\n",
    "from data_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd33f3a0-7dea-4fc1-90a9-71714d216fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.2+cu102\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7655100-1c72-4281-b54d-38eae9b4aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('directory', type=str)\n",
    "# parser.add_argument('output_path', type=str)\n",
    "# args = parser.parse_args()\n",
    "directory = './ml-1m'\n",
    "output_path = './ml-graph-data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d28f25-9f4e-4a8b-ae22-9cb578034446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "users = []\n",
    "with open(os.path.join(directory, 'users.dat'), encoding='latin1') as f:\n",
    "    for l in f:\n",
    "        id_, gender, age, occupation, zip_ = l.strip().split('::')\n",
    "        users.append({\n",
    "            'user_id': int(id_),\n",
    "            'gender': gender,\n",
    "            'age': age,\n",
    "            'occupation': occupation,\n",
    "            'zip': zip_,\n",
    "            })\n",
    "users = pd.DataFrame(users).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6393b321-1dc4-4bde-9484-6abf94cbcc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id gender age occupation    zip\n",
       "0       1      F   1         10  48067\n",
       "1       2      M  56         16  70072\n",
       "2       3      M  25         15  55117\n",
       "3       4      M  45          7  02460\n",
       "4       5      M  25         20  55455"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e63adaa-8067-4792-ae3a-3f987d41cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = []\n",
    "with open(os.path.join(directory, 'movies.dat'), encoding='latin1') as f:\n",
    "    for l in f:\n",
    "        id_, title, genres = l.strip().split('::')\n",
    "        genres_set = set(genres.split('|'))\n",
    "\n",
    "        # extract year\n",
    "        assert re.match(r'.*\\([0-9]{4}\\)$', title)\n",
    "        year = title[-5:-1]\n",
    "        title = title[:-6].strip()\n",
    "\n",
    "        data = {'movie_id': int(id_), 'title': title, 'year': year}\n",
    "        for g in genres_set:\n",
    "            data[g] = True\n",
    "        movies.append(data)\n",
    "movies = pd.DataFrame(movies).astype({'year': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16dd95b-dcc2-4855-a3d5-3c4b6e4ada25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Drama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>1995</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                        title  year Animation Children's Comedy  \\\n",
       "0         1                    Toy Story  1995      True       True   True   \n",
       "1         2                      Jumanji  1995       NaN       True    NaN   \n",
       "2         3             Grumpier Old Men  1995       NaN        NaN   True   \n",
       "3         4            Waiting to Exhale  1995       NaN        NaN   True   \n",
       "4         5  Father of the Bride Part II  1995       NaN        NaN   True   \n",
       "\n",
       "  Fantasy Adventure Romance Drama  \n",
       "0     NaN       NaN     NaN   NaN  \n",
       "1    True      True     NaN   NaN  \n",
       "2     NaN       NaN    True   NaN  \n",
       "3     NaN       NaN     NaN  True  \n",
       "4     NaN       NaN     NaN   NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head().iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc3d87b4-c677-4b5a-8234-305d822f0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []\n",
    "with open(os.path.join(directory, 'ratings.dat'), encoding='latin1') as f:\n",
    "    for l in f:\n",
    "        user_id, movie_id, rating, timestamp = [int(_) for _ in l.split('::')]\n",
    "        ratings.append({\n",
    "            'user_id': user_id,\n",
    "            'movie_id': movie_id,\n",
    "            'rating': rating,\n",
    "            'timestamp': timestamp,\n",
    "            })\n",
    "ratings = pd.DataFrame(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f7d83ca-2f7b-45ed-9e6d-40bcb2b24ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp\n",
       "0        1      1193       5  978300760\n",
       "1        1       661       3  978302109\n",
       "2        1       914       3  978301968\n",
       "3        1      3408       4  978300275\n",
       "4        1      2355       5  978824291"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7067da0-6f4c-4de7-9fa9-95cf4b403325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the users and items that never appear in the rating table.\n",
    "distinct_users_in_ratings = ratings['user_id'].unique()\n",
    "distinct_movies_in_ratings = ratings['movie_id'].unique()\n",
    "users = users[users['user_id'].isin(distinct_users_in_ratings)]\n",
    "movies = movies[movies['movie_id'].isin(distinct_movies_in_ratings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9bdd58b-87d7-4d34-bfa1-49dc6e15fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the movie features into genres (a vector), year (a category), title (a string)\n",
    "genre_columns = movies.columns.drop(['movie_id', 'title', 'year'])\n",
    "movies[genre_columns] = movies[genre_columns].fillna(False).astype('bool')\n",
    "movies_categorical = movies.drop('title', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a56bea90-d78d-4b2d-a1e7-aaf0b7545c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "graph_builder = PandasGraphBuilder()\n",
    "graph_builder.add_entities(users, 'user_id', 'user')\n",
    "graph_builder.add_entities(movies_categorical, 'movie_id', 'movie')\n",
    "graph_builder.add_binary_relations(ratings, 'user_id', 'movie_id', 'watched')\n",
    "graph_builder.add_binary_relations(ratings, 'movie_id', 'user_id', 'watched-by')\n",
    "g = graph_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f83e59cb-7454-4fc7-9cc9-1eca9c9a9eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11910/2390254672.py:3: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  g.nodes['user'].data['gender'] = torch.LongTensor(users['gender'].cat.codes.values)\n"
     ]
    }
   ],
   "source": [
    "# Assign features.\n",
    "# Note that variable-sized features such as texts or images are handled elsewhere.\n",
    "g.nodes['user'].data['gender'] = torch.LongTensor(users['gender'].cat.codes.values)\n",
    "g.nodes['user'].data['age'] = torch.LongTensor(users['age'].cat.codes.values)\n",
    "g.nodes['user'].data['occupation'] = torch.LongTensor(users['occupation'].cat.codes.values)\n",
    "g.nodes['user'].data['zip'] = torch.LongTensor(users['zip'].cat.codes.values)\n",
    "\n",
    "g.nodes['movie'].data['year'] = torch.LongTensor(movies['year'].cat.codes.values)\n",
    "g.nodes['movie'].data['genre'] = torch.FloatTensor(movies[genre_columns].values)\n",
    "\n",
    "g.edges['watched'].data['rating'] = torch.LongTensor(ratings['rating'].values)\n",
    "g.edges['watched'].data['timestamp'] = torch.LongTensor(ratings['timestamp'].values)\n",
    "g.edges['watched-by'].data['rating'] = torch.LongTensor(ratings['rating'].values)\n",
    "g.edges['watched-by'].data['timestamp'] = torch.LongTensor(ratings['timestamp'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f245e3b1-19f2-4708-8c8a-cd486fd2bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/data_utils.py:26: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  df = df.groupby(user, group_keys=False).apply(train_test_split).compute(scheduler='processes').sort_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id  movie_id  rating  timestamp  train_mask  val_mask  test_mask\n",
      "31        1      3186       4  978300019        True     False      False\n",
      "27        1      1721       4  978300055        True     False      False\n",
      "37        1      1022       5  978300055        True     False      False\n",
      "22        1      1270       5  978300055        True     False      False\n",
      "24        1      2340       3  978300103        True     False      False\n",
      "36        1      1836       5  978300172        True     False      False\n",
      "3         1      3408       4  978300275        True     False      False\n",
      "47        1      1207       4  978300719        True     False      False\n",
      "7         1      2804       5  978300719        True     False      False\n",
      "21        1       720       3  978300760        True     False      False\n",
      "0         1      1193       5  978300760        True     False      False\n",
      "44        1       260       4  978300760        True     False      False\n",
      "9         1       919       4  978301368        True     False      False\n",
      "51        1       608       4  978301398        True     False      False\n",
      "43        1      2692       4  978301570        True     False      False\n",
      "41        1      1961       5  978301590        True     False      False\n",
      "48        1      2028       5  978301619        True     False      False\n",
      "18        1      3105       5  978301713        True     False      False\n",
      "11        1       938       4  978301752        True     False      False\n",
      "42        1      1962       4  978301753        True     False      False\n",
      "14        1      1035       5  978301753        True     False      False\n",
      "39        1       150       5  978301777        True     False      False\n",
      "17        1      2018       4  978301777        True     False      False\n",
      "45        1      1028       5  978301777        True     False      False\n",
      "26        1      1097       4  978301953        True     False      False\n",
      "2         1       914       3  978301968        True     False      False\n",
      "19        1      2797       4  978302039        True     False      False\n",
      "6         1      1287       5  978302039        True     False      False\n",
      "38        1      2762       4  978302091        True     False      False\n",
      "52        1      1246       4  978302091        True     False      False\n",
      "1         1       661       3  978302109        True     False      False\n",
      "13        1      2918       4  978302124        True     False      False\n",
      "49        1       531       4  978302149        True     False      False\n",
      "50        1      3114       4  978302174        True     False      False\n",
      "15        1      2791       4  978302188        True     False      False\n",
      "46        1      1029       5  978302205        True     False      False\n",
      "20        1      2321       3  978302205        True     False      False\n",
      "5         1      1197       3  978302268        True     False      False\n",
      "8         1       594       4  978302268        True     False      False\n",
      "12        1      2398       4  978302281        True     False      False\n",
      "28        1      1545       4  978824139        True     False      False\n",
      "23        1       527       5  978824195        True     False      False\n",
      "40        1         1       5  978824268        True     False      False\n",
      "33        1       588       4  978824268        True     False      False\n",
      "16        1      2687       3  978824268        True     False      False\n",
      "29        1       745       3  978824268        True     False      False\n",
      "10        1       595       5  978824268        True     False      False\n",
      "30        1      2294       4  978824291        True     False      False\n",
      "35        1       783       4  978824291        True     False      False\n",
      "4         1      2355       5  978824291        True     False      False\n",
      "34        1      1907       4  978824330        True     False      False\n",
      "32        1      1566       4  978824330       False      True      False\n",
      "25        1        48       5  978824351       False     False       True\n"
     ]
    }
   ],
   "source": [
    "# Train-validation-test split\n",
    "# This is a little bit tricky as we want to select the last interaction for test, and the\n",
    "# second-to-last interaction for validation.\n",
    "train_indices, val_indices, test_indices = train_test_split_by_time(ratings, 'timestamp', 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "596103b7-8447-4768-bfd7-24c0759e481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph with training interactions only.\n",
    "train_g = build_train_graph(g, train_indices, 'user', 'movie', 'watched', 'watched-by')\n",
    "assert train_g.out_degrees(etype='watched').min() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35070a29-2a46-46ae-8ac2-5bd6f5c427bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the user-item sparse matrix for validation and test set.\n",
    "val_matrix, test_matrix = build_val_test_matrix(g, val_indices, test_indices, 'user', 'movie', 'watched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24096d7d-b68c-4ed6-be32-44b4f1099f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build title set\n",
    "movie_textual_dataset = {'title': movies['title'].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dd8065e-b0f2-456d-8537-0b56f4f2b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dump the graph and the datasets\n",
    "dataset = {\n",
    "    'train-graph': train_g,\n",
    "    'val-matrix': val_matrix,\n",
    "    'test-matrix': test_matrix,\n",
    "    'item-texts': movie_textual_dataset,\n",
    "    'item-images': None,\n",
    "    'user-type': 'user',\n",
    "    'item-type': 'movie',\n",
    "    'user-to-item-type': 'watched',\n",
    "    'item-to-user-type': 'watched-by',\n",
    "    'timestamp-edge-column': 'timestamp'}\n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3638d990-6f78-4912-a16f-07ccf0af6460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluation.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import dgl\n",
    "import argparse\n",
    "\n",
    "def prec(recommendations, ground_truth):\n",
    "    n_users, n_items = ground_truth.shape\n",
    "    K = recommendations.shape[1]\n",
    "    user_idx = np.repeat(np.arange(n_users), K)\n",
    "    item_idx = recommendations.flatten()\n",
    "    relevance = ground_truth[user_idx, item_idx].reshape((n_users, K))\n",
    "    hit = relevance.any(axis=1).mean()\n",
    "    return hit\n",
    "\n",
    "class LatestNNRecommender(object):\n",
    "    def __init__(self, user_ntype, item_ntype, user_to_item_etype, timestamp, batch_size):\n",
    "        self.user_ntype = user_ntype\n",
    "        self.item_ntype = item_ntype\n",
    "        self.user_to_item_etype = user_to_item_etype\n",
    "        self.batch_size = batch_size\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "    def recommend(self, full_graph, K, h_user, h_item):\n",
    "        \"\"\"\n",
    "        Return a (n_user, K) matrix of recommended items for each user\n",
    "        \"\"\"\n",
    "        graph_slice = full_graph.edge_type_subgraph([self.user_to_item_etype])\n",
    "        n_users = full_graph.number_of_nodes(self.user_ntype)\n",
    "        latest_interactions = dgl.sampling.select_topk(graph_slice, 1, self.timestamp, edge_dir='out')\n",
    "        user, latest_items = latest_interactions.all_edges(form='uv', order='srcdst')\n",
    "        # each user should have at least one \"latest\" interaction\n",
    "        assert torch.equal(user, torch.arange(n_users))\n",
    "\n",
    "        recommended_batches = []\n",
    "        user_batches = torch.arange(n_users).split(self.batch_size)\n",
    "        for user_batch in user_batches:\n",
    "            latest_item_batch = latest_items[user_batch].to(device=h_item.device)\n",
    "            dist = h_item[latest_item_batch] @ h_item.t()\n",
    "            # exclude items that are already interacted\n",
    "            for i, u in enumerate(user_batch.tolist()):\n",
    "                interacted_items = full_graph.successors(u, etype=self.user_to_item_etype)\n",
    "                dist[i, interacted_items] = -np.inf\n",
    "            recommended_batches.append(dist.topk(K, 1)[1])\n",
    "\n",
    "        recommendations = torch.cat(recommended_batches, 0)\n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def evaluate_nn(dataset, h_item, k, batch_size):\n",
    "    g = dataset['train-graph']\n",
    "    val_matrix = dataset['val-matrix'].tocsr()\n",
    "    test_matrix = dataset['test-matrix'].tocsr()\n",
    "    item_texts = dataset['item-texts']\n",
    "    user_ntype = dataset['user-type']\n",
    "    item_ntype = dataset['item-type']\n",
    "    user_to_item_etype = dataset['user-to-item-type']\n",
    "    timestamp = dataset['timestamp-edge-column']\n",
    "\n",
    "    rec_engine = LatestNNRecommender(\n",
    "        user_ntype, item_ntype, user_to_item_etype, timestamp, batch_size)\n",
    "\n",
    "    recommendations = rec_engine.recommend(g, k, None, h_item).cpu().numpy()\n",
    "    return prec(recommendations, val_matrix)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('dataset_path', type=str)\n",
    "    parser.add_argument('item_embedding_path', type=str)\n",
    "    parser.add_argument('-k', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=32)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.dataset_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    with open(args.item_embedding_path, 'rb') as f:\n",
    "        emb = torch.FloatTensor(pickle.load(f))\n",
    "    print(evaluate_nn(dataset, emb, args.k, args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d3a77c6-6e3d-4cc2-8264-4546f22fd6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile layers.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import dgl.function as fn\n",
    "\n",
    "def disable_grad(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def _init_input_modules(g, ntype, textset, hidden_dims):\n",
    "    # We initialize the linear projections of each input feature ``x`` as\n",
    "    # follows:\n",
    "    # * If ``x`` is a scalar integral feature, we assume that ``x`` is a categorical\n",
    "    #   feature, and assume the range of ``x`` is 0..max(x).\n",
    "    # * If ``x`` is a float one-dimensional feature, we assume that ``x`` is a\n",
    "    #   numeric vector.\n",
    "    # * If ``x`` is a field of a textset, we process it as bag of words.\n",
    "    module_dict = nn.ModuleDict()\n",
    "\n",
    "    for column, data in g.nodes[ntype].data.items():\n",
    "        if column == dgl.NID:\n",
    "            continue\n",
    "        if data.dtype == torch.float32:\n",
    "            assert data.ndim == 2\n",
    "            m = nn.Linear(data.shape[1], hidden_dims)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            module_dict[column] = m\n",
    "        elif data.dtype == torch.int64:\n",
    "            assert data.ndim == 1\n",
    "            m = nn.Embedding(\n",
    "                data.max() + 2, hidden_dims, padding_idx=-1)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            module_dict[column] = m\n",
    "\n",
    "    if textset is not None:\n",
    "        for column, field in textset.fields.items():\n",
    "            if field.vocab.vectors:\n",
    "                module_dict[column] = BagOfWordsPretrained(field, hidden_dims)\n",
    "            else:\n",
    "                module_dict[column] = BagOfWords(field, hidden_dims)\n",
    "\n",
    "    return module_dict\n",
    "\n",
    "class BagOfWordsPretrained(nn.Module):\n",
    "    def __init__(self, field, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        input_dims = field.vocab.vectors.shape[1]\n",
    "        self.emb = nn.Embedding(\n",
    "            len(field.vocab.itos), input_dims,\n",
    "            padding_idx=field.vocab.stoi[field.pad_token])\n",
    "        self.emb.weight[:] = field.vocab.vectors\n",
    "        self.proj = nn.Linear(input_dims, hidden_dims)\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "        nn.init.constant_(self.proj.bias, 0)\n",
    "\n",
    "        disable_grad(self.emb)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        \"\"\"\n",
    "        x: (batch_size, max_length) LongTensor\n",
    "        length: (batch_size,) LongTensor\n",
    "        \"\"\"\n",
    "        x = self.emb(x).sum(1) / length.unsqueeze(1).float()\n",
    "        return self.proj(x)\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, field, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(\n",
    "            len(field.vocab.itos), hidden_dims,\n",
    "            padding_idx=field.vocab.stoi[field.pad_token])\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        return self.emb(x).sum(1) / length.unsqueeze(1).float()\n",
    "\n",
    "class LinearProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects each input feature of the graph linearly and sums them up\n",
    "    \"\"\"\n",
    "    def __init__(self, full_graph, ntype, textset, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ntype = ntype\n",
    "        self.inputs = _init_input_modules(full_graph, ntype, textset, hidden_dims)\n",
    "\n",
    "    def forward(self, ndata):\n",
    "        projections = []\n",
    "        for feature, data in ndata.items():\n",
    "            if feature == dgl.NID or feature.endswith('__len'):\n",
    "                # This is an additional feature indicating the length of the ``feature``\n",
    "                # column; we shouldn't process this.\n",
    "                continue\n",
    "\n",
    "            module = self.inputs[feature]\n",
    "            if isinstance(module, (BagOfWords, BagOfWordsPretrained)):\n",
    "                # Textual feature; find the length and pass it to the textual module.\n",
    "                length = ndata[feature + '__len']\n",
    "                result = module(data, length)\n",
    "            else:\n",
    "                result = module(data)\n",
    "            projections.append(result)\n",
    "\n",
    "        return torch.stack(projections, 1).sum(1)\n",
    "\n",
    "class WeightedSAGEConv(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, act=F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = act\n",
    "        self.Q = nn.Linear(input_dims, hidden_dims)\n",
    "        self.W = nn.Linear(input_dims + hidden_dims, output_dims)\n",
    "        self.reset_parameters()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.Q.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.W.weight, gain=gain)\n",
    "        nn.init.constant_(self.Q.bias, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "    def forward(self, g, h, weights):\n",
    "        \"\"\"\n",
    "        g : graph\n",
    "        h : node features\n",
    "        weights : scalar edge weights\n",
    "        \"\"\"\n",
    "        h_src, h_dst = h\n",
    "        with g.local_scope():\n",
    "            g.srcdata['n'] = self.act(self.Q(self.dropout(h_src)))\n",
    "            g.edata['w'] = weights.float()\n",
    "            g.update_all(fn.u_mul_e('n', 'w', 'm'), fn.sum('m', 'n'))\n",
    "            g.update_all(fn.copy_e('w', 'm'), fn.sum('m', 'ws'))\n",
    "            n = g.dstdata['n']\n",
    "            ws = g.dstdata['ws'].unsqueeze(1).clamp(min=1)\n",
    "            z = self.act(self.W(self.dropout(torch.cat([n / ws, h_dst], 1))))\n",
    "            z_norm = z.norm(2, 1, keepdim=True)\n",
    "            z_norm = torch.where(z_norm == 0, torch.tensor(1.).to(z_norm), z_norm)\n",
    "            z = z / z_norm\n",
    "            return z\n",
    "\n",
    "class SAGENet(nn.Module):\n",
    "    def __init__(self, hidden_dims, n_layers):\n",
    "        \"\"\"\n",
    "        g : DGLHeteroGraph\n",
    "            The user-item interaction graph.\n",
    "            This is only for finding the range of categorical variables.\n",
    "        item_textsets : torchtext.data.Dataset\n",
    "            The textual features of each item node.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.convs.append(WeightedSAGEConv(hidden_dims, hidden_dims, hidden_dims))\n",
    "\n",
    "    def forward(self, blocks, h):\n",
    "        for layer, block in zip(self.convs, blocks):\n",
    "            h_dst = h[:block.number_of_nodes('DST/' + block.ntypes[0])]\n",
    "            h = layer(block, (h, h_dst), block.edata['weights'])\n",
    "        return h\n",
    "\n",
    "class ItemToItemScorer(nn.Module):\n",
    "    def __init__(self, full_graph, ntype):\n",
    "        super().__init__()\n",
    "\n",
    "        n_nodes = full_graph.number_of_nodes(ntype)\n",
    "        self.bias = nn.Parameter(torch.zeros(n_nodes))\n",
    "\n",
    "    def _add_bias(self, edges):\n",
    "        bias_src = self.bias[edges.src[dgl.NID]]\n",
    "        bias_dst = self.bias[edges.dst[dgl.NID]]\n",
    "        return {'s': edges.data['s'] + bias_src + bias_dst}\n",
    "\n",
    "    def forward(self, item_item_graph, h):\n",
    "        \"\"\"\n",
    "        item_item_graph : graph consists of edges connecting the pairs\n",
    "        h : hidden state of every node\n",
    "        \"\"\"\n",
    "        with item_item_graph.local_scope():\n",
    "            item_item_graph.ndata['h'] = h\n",
    "            item_item_graph.apply_edges(fn.u_dot_v('h', 'h', 's'))\n",
    "            item_item_graph.apply_edges(self._add_bias)\n",
    "            pair_score = item_item_graph.edata['s']\n",
    "        return pair_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79c82960-d923-49e0-b0c9-5871e2903137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sampler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sampler.py\n",
    "\n",
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "def compact_and_copy(frontier, seeds):\n",
    "    block = dgl.to_block(frontier, seeds)\n",
    "    for col, data in frontier.edata.items():\n",
    "        if col == dgl.EID:\n",
    "            continue\n",
    "        block.edata[col] = data[block.edata[dgl.EID]]\n",
    "    return block\n",
    "\n",
    "class ItemToItemBatchSampler(IterableDataset):\n",
    "    def __init__(self, g, user_type, item_type, batch_size):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            heads = torch.randint(0, self.g.number_of_nodes(self.item_type), (self.batch_size,))\n",
    "            tails = dgl.sampling.random_walk(\n",
    "                self.g,\n",
    "                heads,\n",
    "                metapath=[self.item_to_user_etype, self.user_to_item_etype])[0][:, 2]\n",
    "            neg_tails = torch.randint(0, self.g.number_of_nodes(self.item_type), (self.batch_size,))\n",
    "\n",
    "            mask = (tails != -1)\n",
    "            yield heads[mask], tails[mask], neg_tails[mask]\n",
    "\n",
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, user_type, item_type, random_walk_length, random_walk_restart_prob,\n",
    "                 num_random_walks, num_neighbors, num_layers):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.samplers = [\n",
    "            dgl.sampling.PinSAGESampler(g, item_type, user_type, random_walk_length,\n",
    "                random_walk_restart_prob, num_random_walks, num_neighbors)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "    def sample_blocks(self, seeds, heads=None, tails=None, neg_tails=None):\n",
    "        blocks = []\n",
    "        for sampler in self.samplers:\n",
    "            frontier = sampler(seeds)\n",
    "            if heads is not None:\n",
    "                eids = frontier.edge_ids(torch.cat([heads, heads]), torch.cat([tails, neg_tails]), return_uv=True)[2]\n",
    "                if len(eids) > 0:\n",
    "                    old_frontier = frontier\n",
    "                    frontier = dgl.remove_edges(old_frontier, eids)\n",
    "                    #print(old_frontier)\n",
    "                    #print(frontier)\n",
    "                    #print(frontier.edata['weights'])\n",
    "                    #frontier.edata['weights'] = old_frontier.edata['weights'][frontier.edata[dgl.EID]]\n",
    "            block = compact_and_copy(frontier, seeds)\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "            blocks.insert(0, block)\n",
    "        return blocks\n",
    "\n",
    "    def sample_from_item_pairs(self, heads, tails, neg_tails):\n",
    "        # Create a graph with positive connections only and another graph with negative\n",
    "        # connections only.\n",
    "        pos_graph = dgl.graph(\n",
    "            (heads, tails),\n",
    "            num_nodes=self.g.number_of_nodes(self.item_type))\n",
    "        neg_graph = dgl.graph(\n",
    "            (heads, neg_tails),\n",
    "            num_nodes=self.g.number_of_nodes(self.item_type))\n",
    "        pos_graph, neg_graph = dgl.compact_graphs([pos_graph, neg_graph])\n",
    "        seeds = pos_graph.ndata[dgl.NID]\n",
    "\n",
    "        blocks = self.sample_blocks(seeds, heads, tails, neg_tails)\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "def assign_simple_node_features(ndata, g, ntype, assign_id=False):\n",
    "    \"\"\"\n",
    "    Copies data to the given block from the corresponding nodes in the original graph.\n",
    "    \"\"\"\n",
    "    for col in g.nodes[ntype].data.keys():\n",
    "        if not assign_id and col == dgl.NID:\n",
    "            continue\n",
    "        induced_nodes = ndata[dgl.NID]\n",
    "        ndata[col] = g.nodes[ntype].data[col][induced_nodes]\n",
    "\n",
    "def assign_textual_node_features(ndata, textset, ntype):\n",
    "    \"\"\"\n",
    "    Assigns numericalized tokens from a torchtext dataset to given block.\n",
    "\n",
    "    The numericalized tokens would be stored in the block as node features\n",
    "    with the same name as ``field_name``.\n",
    "\n",
    "    The length would be stored as another node feature with name\n",
    "    ``field_name + '__len'``.\n",
    "\n",
    "    block : DGLHeteroGraph\n",
    "        First element of the compacted blocks, with \"dgl.NID\" as the\n",
    "        corresponding node ID in the original graph, hence the index to the\n",
    "        text dataset.\n",
    "\n",
    "        The numericalized tokens (and lengths if available) would be stored\n",
    "        onto the blocks as new node features.\n",
    "    textset : torchtext.data.Dataset\n",
    "        A torchtext dataset whose number of examples is the same as that\n",
    "        of nodes in the original graph.\n",
    "    \"\"\"\n",
    "    node_ids = ndata[dgl.NID].numpy()\n",
    "\n",
    "    for field_name, field in textset.fields.items():\n",
    "        examples = [getattr(textset[i], field_name) for i in node_ids]\n",
    "\n",
    "        tokens, lengths = field.process(examples)\n",
    "\n",
    "        if not field.batch_first:\n",
    "            tokens = tokens.t()\n",
    "\n",
    "        ndata[field_name] = tokens\n",
    "        ndata[field_name + '__len'] = lengths\n",
    "\n",
    "def assign_features_to_blocks(blocks, g, textset, ntype):\n",
    "    # For the first block (which is closest to the input), copy the features from\n",
    "    # the original graph as well as the texts.\n",
    "    assign_simple_node_features(blocks[0].srcdata, g, ntype)\n",
    "    assign_textual_node_features(blocks[0].srcdata, textset, ntype)\n",
    "    assign_simple_node_features(blocks[-1].dstdata, g, ntype)\n",
    "    assign_textual_node_features(blocks[-1].dstdata, textset, ntype)\n",
    "\n",
    "class PinSAGECollator(object):\n",
    "    def __init__(self, sampler, g, ntype, textset):\n",
    "        self.sampler = sampler\n",
    "        self.ntype = ntype\n",
    "        self.g = g\n",
    "        self.textset = textset\n",
    "\n",
    "    def collate_train(self, batches):\n",
    "        heads, tails, neg_tails = batches[0]\n",
    "        # Construct multilayer neighborhood via PinSAGE...\n",
    "        pos_graph, neg_graph, blocks = self.sampler.sample_from_item_pairs(heads, tails, neg_tails)\n",
    "        assign_features_to_blocks(blocks, self.g, self.textset, self.ntype)\n",
    "\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "    def collate_test(self, samples):\n",
    "        batch = torch.LongTensor(samples)\n",
    "        blocks = self.sampler.sample_blocks(batch)\n",
    "        assign_features_to_blocks(blocks, self.g, self.textset, self.ntype)\n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "402e8007-8198-4a81-b716-6b9dd9d177b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "#import torchtext\n",
    "import dgl\n",
    "import tqdm\n",
    "\n",
    "import layers\n",
    "import sampler as sampler_module\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83e07b72-0ae5-414f-8daa-322910dd2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinSAGEModel(nn.Module):\n",
    "    def __init__(self, full_graph, ntype, textsets, hidden_dims, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = layers.LinearProjector(full_graph, ntype, textsets, hidden_dims)\n",
    "        self.sage = layers.SAGENet(hidden_dims, n_layers)\n",
    "        self.scorer = layers.ItemToItemScorer(full_graph, ntype)\n",
    "\n",
    "    def forward(self, pos_graph, neg_graph, blocks):\n",
    "        h_item = self.get_repr(blocks)\n",
    "        pos_score = self.scorer(pos_graph, h_item)\n",
    "        neg_score = self.scorer(neg_graph, h_item)\n",
    "        return (neg_score - pos_score + 1).clamp(min=0)\n",
    "\n",
    "    def get_repr(self, blocks):\n",
    "        h_item = self.proj(blocks[0].srcdata)\n",
    "        h_item_dst = self.proj(blocks[-1].dstdata)\n",
    "        return h_item_dst + self.sage(blocks, h_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11b4bb9b-9be5-47f4-866b-37d5646984c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset_path', type=str, default='./ml-graph-data.pkl')\n",
    "parser.add_argument('--random-walk-length', type=int, default=2)\n",
    "parser.add_argument('--random-walk-restart-prob', type=float, default=0.5)\n",
    "parser.add_argument('--num-random-walks', type=int, default=10)\n",
    "parser.add_argument('--num-neighbors', type=int, default=3)\n",
    "parser.add_argument('--num-layers', type=int, default=2)\n",
    "parser.add_argument('--hidden-dims', type=int, default=16)\n",
    "parser.add_argument('--batch-size', type=int, default=32)\n",
    "parser.add_argument('--device', type=str, default='cpu')        # can also be \"cuda:0\"\n",
    "parser.add_argument('--num-epochs', type=int, default=1)\n",
    "parser.add_argument('--batches-per-epoch', type=int, default=20000)\n",
    "parser.add_argument('--num-workers', type=int, default=0)\n",
    "parser.add_argument('--lr', type=float, default=3e-5)\n",
    "parser.add_argument('-k', type=int, default=10)\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56a2b246-8992-46d2-9802-6a7d8091802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open(args.dataset_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8af9c5a-44d7-4918-9ceb-5b236b263b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "978b428f-18f4-42a8-95ee-28398ca5462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63cdb201-b2e8-461b-b6b3-a2df53de70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bdb87c-42a6-4ccc-bada-72189b15ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48e40998-3467-4f40-ad64-48e81db77eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare torchtext dataset and vocabulary\n",
    "fields = {}\n",
    "examples = []\n",
    "for key, texts in item_texts.items():\n",
    "    fields[key] = torchtext.legacy.data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "for i in range(g.number_of_nodes(item_ntype)):\n",
    "    example = torchtext.legacy.data.Example.fromlist(\n",
    "        [item_texts[key][i] for key in item_texts.keys()],\n",
    "        [(key, fields[key]) for key in item_texts.keys()])\n",
    "    examples.append(example)\n",
    "textset = torchtext.legacy.data.Dataset(examples, fields)\n",
    "for key, field in fields.items():\n",
    "    field.build_vocab(getattr(textset, key))\n",
    "    #field.build_vocab(getattr(textset, key), vectors='fasttext.simple.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7208eee7-e98e-4f09-b0d4-642ebefe9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler\n",
    "batch_sampler = sampler_module.ItemToItemBatchSampler(g, user_ntype, item_ntype, args.batch_size)\n",
    "neighbor_sampler = sampler_module.NeighborSampler(g, user_ntype, item_ntype, args.random_walk_length,\n",
    "                                                  args.random_walk_restart_prob, args.num_random_walks,\n",
    "                                                  args.num_neighbors, args.num_layers)\n",
    "collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "dataloader = DataLoader(batch_sampler, collate_fn=collator.collate_train, num_workers=args.num_workers)\n",
    "dataloader_test = DataLoader(torch.arange(g.number_of_nodes(item_ntype)), batch_size=args.batch_size,\n",
    "                             collate_fn=collator.collate_test, num_workers=args.num_workers)\n",
    "dataloader_it = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7bd8423-a9d1-46b1-b278-825f8926e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = PinSAGEModel(g, item_ntype, textset, args.hidden_dims, args.num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae2fcdbf-fd7f-492d-871c-54fa5205435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c045c4f3-89cf-481c-8b43-2ce1bec02ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [09:57<00:00, 33.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# For each batch of head-tail-negative triplets...\n",
    "for epoch_id in range(args.num_epochs):\n",
    "    model.train()\n",
    "    for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "        pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "        # Copy to GPU\n",
    "        for i in range(len(blocks)):\n",
    "            blocks[i] = blocks[i].to(device)\n",
    "        pos_graph = pos_graph.to(device)\n",
    "        neg_graph = neg_graph.to(device)\n",
    "\n",
    "        loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7ea5c86-a43f-4c4c-9920-5c69c9abd95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028973509933774833\n"
     ]
    }
   ],
   "source": [
    "# Evaluate HIT@10\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "    h_item_batches = []\n",
    "    for blocks in dataloader_test:\n",
    "        for i in range(len(blocks)):\n",
    "            blocks[i] = blocks[i].to(device)\n",
    "\n",
    "        h_item_batches.append(model.get_repr(blocks))\n",
    "    h_item = torch.cat(h_item_batches, 0)\n",
    "\n",
    "    print(evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcdbc40-1e39-4628-bbaf-8960445d674f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
